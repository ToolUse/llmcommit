{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Blueprint docs","text":"<p>Blueprints are customizable workflows that help developers build AI applications using open-source tools and models</p> <p>These docs are your companion to mastering this Blueprint.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Tool 1</li> <li>Tool 2</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-the-blueprint-in-minutes","title":"Start building the Blueprint in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor project parameters to fit your needs</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#ai-service-module","title":"AI Service Module","text":""},{"location":"api/#blueprint.ai_service","title":"<code>blueprint.ai_service</code>","text":"<p>AI Service module for interacting with different LLM providers.</p>"},{"location":"api/#blueprint.ai_service.AIService","title":"<code>AIService</code>","text":"<p>Service for interacting with different AI providers.</p> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>class AIService:\n    \"\"\"Service for interacting with different AI providers.\"\"\"\n\n    def __init__(self, service_type: str, model: Optional[str] = None):\n        \"\"\"Initialize AI service.\n\n        Args:\n            service_type: Type of AI service ('ollama' or 'jan')\n            model: Model name to use\n        \"\"\"\n        self.service_type = service_type\n        self.model = model\n\n        # Set up base URLs for services\n        self.base_urls = {\n            \"ollama\": \"http://localhost:11434/api/generate\",\n            \"jan\": \"http://localhost:1337/v1/chat/completions\",\n        }\n\n        if service_type not in self.base_urls:\n            raise ValueError(f\"Unsupported service type: {service_type}\")\n\n    def query(self, prompt: str) -&gt; str:\n        \"\"\"Query the AI service with the given prompt.\n\n        Args:\n            prompt: The prompt to send to the AI service\n\n        Returns:\n            The response from the AI service\n\n        Raises:\n            Exception: If there's an error communicating with the AI service\n        \"\"\"\n        if self.service_type == \"ollama\":\n            return self._query_ollama(prompt)\n        elif self.service_type == \"jan\":\n            return self._query_jan(prompt)\n        else:\n            raise ValueError(f\"Unsupported service type: {self.service_type}\")\n\n    def _query_ollama(self, prompt: str) -&gt; str:\n        \"\"\"Send query to Ollama API.\n\n        Args:\n            prompt: The prompt text\n\n        Returns:\n            Generated text response\n        \"\"\"\n        url = self.base_urls[\"ollama\"]\n        data = {\"model\": self.model, \"prompt\": prompt, \"stream\": False}\n\n        try:\n            response = requests.post(url, json=data)\n            response.raise_for_status()\n            return response.json().get(\"response\", \"\")\n        except Exception as e:\n            raise Exception(f\"Error querying Ollama API: {e}\")\n\n    def _query_jan(self, prompt: str) -&gt; str:\n        \"\"\"Send query to Jan AI API.\n\n        Args:\n            prompt: The prompt text\n\n        Returns:\n            Generated text response\n        \"\"\"\n        url = self.base_urls[\"jan\"]\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = {\n            \"model\": self.model,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            \"temperature\": 0.7,\n        }\n\n        try:\n            response = requests.post(url, headers=headers, json=data)\n            response.raise_for_status()\n            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n        except Exception as e:\n            raise Exception(f\"Error querying Jan AI API: {e}\")\n</code></pre>"},{"location":"api/#blueprint.ai_service.AIService.__init__","title":"<code>__init__(service_type, model=None)</code>","text":"<p>Initialize AI service.</p> <p>Parameters:</p> Name Type Description Default <code>service_type</code> <code>str</code> <p>Type of AI service ('ollama' or 'jan')</p> required <code>model</code> <code>Optional[str]</code> <p>Model name to use</p> <code>None</code> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>def __init__(self, service_type: str, model: Optional[str] = None):\n    \"\"\"Initialize AI service.\n\n    Args:\n        service_type: Type of AI service ('ollama' or 'jan')\n        model: Model name to use\n    \"\"\"\n    self.service_type = service_type\n    self.model = model\n\n    # Set up base URLs for services\n    self.base_urls = {\n        \"ollama\": \"http://localhost:11434/api/generate\",\n        \"jan\": \"http://localhost:1337/v1/chat/completions\",\n    }\n\n    if service_type not in self.base_urls:\n        raise ValueError(f\"Unsupported service type: {service_type}\")\n</code></pre>"},{"location":"api/#blueprint.ai_service.AIService.query","title":"<code>query(prompt)</code>","text":"<p>Query the AI service with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the AI service</p> required <p>Returns:</p> Type Description <code>str</code> <p>The response from the AI service</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error communicating with the AI service</p> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>def query(self, prompt: str) -&gt; str:\n    \"\"\"Query the AI service with the given prompt.\n\n    Args:\n        prompt: The prompt to send to the AI service\n\n    Returns:\n        The response from the AI service\n\n    Raises:\n        Exception: If there's an error communicating with the AI service\n    \"\"\"\n    if self.service_type == \"ollama\":\n        return self._query_ollama(prompt)\n    elif self.service_type == \"jan\":\n        return self._query_jan(prompt)\n    else:\n        raise ValueError(f\"Unsupported service type: {self.service_type}\")\n</code></pre>"},{"location":"api/#commit-generator-module","title":"Commit Generator Module","text":""},{"location":"api/#blueprint.commit_generator","title":"<code>blueprint.commit_generator</code>","text":"<p>Git commit message generator using AI models.</p>"},{"location":"api/#blueprint.commit_generator.create_commit","title":"<code>create_commit(message)</code>","text":"<p>Create a git commit with the selected message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Commit message to use</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if commit was successful, False otherwise</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def create_commit(message: str) -&gt; bool:\n    \"\"\"Create a git commit with the selected message.\n\n    Args:\n        message: Commit message to use\n\n    Returns:\n        True if commit was successful, False otherwise\n    \"\"\"\n    try:\n        subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n        print(f\"Committed with message: {message}\")\n        return True\n    except subprocess.CalledProcessError:\n        print(\"Error: Failed to create commit.\")\n        return False\n</code></pre>"},{"location":"api/#blueprint.commit_generator.generate_commit_messages","title":"<code>generate_commit_messages(diff, max_chars=75, service_type='ollama', ollama_model='llama3.1', jan_model='Llama 3.1')</code>","text":"<p>Generate commit messages based on git diff.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>str</code> <p>Git diff to generate commit messages for</p> required <code>max_chars</code> <code>int</code> <p>Suggested maximum characters for commit messages</p> <code>75</code> <code>service_type</code> <code>str</code> <p>'ollama' or 'jan'</p> <code>'ollama'</code> <code>ollama_model</code> <code>str</code> <p>Model name for Ollama</p> <code>'llama3.1'</code> <code>jan_model</code> <code>str</code> <p>Model name for Jan AI</p> <code>'Llama 3.1'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated commit messages</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def generate_commit_messages(\n    diff: str,\n    max_chars: int = 75,\n    service_type: str = \"ollama\",\n    ollama_model: str = \"llama3.1\",\n    jan_model: str = \"Llama 3.1\",\n) -&gt; List[str]:\n    \"\"\"Generate commit messages based on git diff.\n\n    Args:\n        diff: Git diff to generate commit messages for\n        max_chars: Suggested maximum characters for commit messages\n        service_type: 'ollama' or 'jan'\n        ollama_model: Model name for Ollama\n        jan_model: Model name for Jan AI\n\n    Returns:\n        List of generated commit messages\n    \"\"\"\n    prompt = f\"\"\"\n    Your task is to generate three concise, informative git commit messages based on the following git diff.\n    Be sure that each commit message reflects the entire diff.\n    It is very important that the entire commit is clear and understandable with each of the three options. \n    Try to fit each commit message in {max_chars} characters.\n    Each message should be on a new line, starting with a number and a period (e.g., '1.', '2.', '3.').\n    Here's the diff:\\n\\n{diff}\"\"\"\n\n    response = query_ai_service(prompt, service_type, ollama_model, jan_model)\n    return parse_commit_messages(response)\n</code></pre>"},{"location":"api/#blueprint.commit_generator.get_git_diff","title":"<code>get_git_diff(max_chars=5000)</code>","text":"<p>Get the git diff of staged changes, or unstaged if no staged changes.</p> <p>Parameters:</p> Name Type Description Default <code>max_chars</code> <code>int</code> <p>Maximum number of characters to return</p> <code>5000</code> <p>Returns:</p> Type Description <code>str</code> <p>Git diff as string</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If not a git repository or git not installed</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def get_git_diff(max_chars: int = 5000) -&gt; str:\n    \"\"\"Get the git diff of staged changes, or unstaged if no staged changes.\n\n    Args:\n        max_chars: Maximum number of characters to return\n\n    Returns:\n        Git diff as string\n\n    Raises:\n        SystemExit: If not a git repository or git not installed\n    \"\"\"\n    try:\n        diff = subprocess.check_output([\"git\", \"diff\", \"--cached\"], text=True)\n        if not diff:\n            diff = subprocess.check_output([\"git\", \"diff\"], text=True)\n        return diff[:max_chars]  # Limit to max_chars characters\n    except subprocess.CalledProcessError:\n        print(\"Error: Not a git repository or git is not installed.\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#blueprint.commit_generator.parse_commit_messages","title":"<code>parse_commit_messages(response)</code>","text":"<p>Parse the LLM response into a list of commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Text response from AI service</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of extracted commit messages</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def parse_commit_messages(response: str) -&gt; List[str]:\n    \"\"\"Parse the LLM response into a list of commit messages.\n\n    Args:\n        response: Text response from AI service\n\n    Returns:\n        List of extracted commit messages\n    \"\"\"\n    messages = []\n    for line in response.split(\"\\n\"):\n        if line.strip().startswith((\"1.\", \"2.\", \"3.\")):\n            messages.append(line.split(\".\", 1)[1].strip())\n    return messages\n</code></pre>"},{"location":"api/#blueprint.commit_generator.query_ai_service","title":"<code>query_ai_service(prompt, service_type, ollama_model, jan_model)</code>","text":"<p>Query AI service with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt text to send to AI service</p> required <code>service_type</code> <code>str</code> <p>Type of AI service ('ollama' or 'jan')</p> required <code>ollama_model</code> <code>str</code> <p>Model name for Ollama</p> required <code>jan_model</code> <code>str</code> <p>Model name for Jan AI</p> required <p>Returns:</p> Type Description <code>str</code> <p>Response from AI service</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If there's an error querying the AI service</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def query_ai_service(\n    prompt: str, service_type: str, ollama_model: str, jan_model: str\n) -&gt; str:\n    \"\"\"Query AI service with the given prompt.\n\n    Args:\n        prompt: Prompt text to send to AI service\n        service_type: Type of AI service ('ollama' or 'jan')\n        ollama_model: Model name for Ollama\n        jan_model: Model name for Jan AI\n\n    Returns:\n        Response from AI service\n\n    Raises:\n        SystemExit: If there's an error querying the AI service\n    \"\"\"\n    try:\n        print(\"Generating commit messages...\", end=\"\", flush=True)\n        ai_service = AIService(\n            service_type, model=ollama_model if service_type == \"ollama\" else jan_model\n        )\n        response = ai_service.query(prompt)\n        print(\"Done!\")\n        return response\n    except Exception as e:\n        print(f\"\\nError querying {service_type.capitalize()}: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#blueprint.commit_generator.select_message_with_fzf","title":"<code>select_message_with_fzf(messages, use_vim=False, use_num=False)</code>","text":"<p>Use fzf to select a commit message, with option to regenerate.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[str]</code> <p>List of commit messages to select from</p> required <code>use_vim</code> <code>bool</code> <p>Whether to use vim-style navigation</p> <code>False</code> <code>use_num</code> <code>bool</code> <p>Whether to display numbers for selection</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Selected message, \"regenerate\" to regenerate messages, or None if cancelled</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def select_message_with_fzf(\n    messages: List[str], use_vim: bool = False, use_num: bool = False\n) -&gt; Optional[str]:\n    \"\"\"Use fzf to select a commit message, with option to regenerate.\n\n    Args:\n        messages: List of commit messages to select from\n        use_vim: Whether to use vim-style navigation\n        use_num: Whether to display numbers for selection\n\n    Returns:\n        Selected message, \"regenerate\" to regenerate messages, or None if cancelled\n    \"\"\"\n    try:\n        messages.append(\"Regenerate messages\")\n        fzf_args = [\n            \"fzf\",\n            \"--height=10\",\n            \"--layout=reverse\",\n            \"--prompt=Select a commit message (ESC to cancel): \",\n            \"--no-info\",\n            \"--margin=1,2\",\n            \"--border\",\n            \"--color=prompt:#D73BC9,pointer:#D73BC9\",\n        ]\n\n        if use_vim:\n            fzf_args.extend([\"--bind\", \"j:down,k:up\"])\n\n        if use_num:\n            for i, msg in enumerate(messages):\n                messages[i] = f\"{i+1}. {msg}\"\n            fzf_args.extend(\n                [\n                    \"--bind\",\n                    \"1:accept-non-empty,2:accept-non-empty,3:accept-non-empty,4:accept-non-empty\",\n                ]\n            )\n\n        result = subprocess.run(\n            fzf_args,\n            input=\"\\n\".join(messages),\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode == 130:  # User pressed ESC\n            return None\n        selected = result.stdout.strip()\n        if selected == \"Regenerate messages\" or selected == \"4. Regenerate messages\":\n            return \"regenerate\"\n        return selected.split(\". \", 1)[1] if use_num and selected else selected\n    except subprocess.CalledProcessError:\n        print(\"Error: fzf selection failed.\")\n        return None\n</code></pre>"},{"location":"api/#cli-module","title":"CLI Module","text":""},{"location":"api/#blueprint.cli","title":"<code>blueprint.cli</code>","text":"<p>Command-line interface for LLM-powered commit message generator.</p>"},{"location":"api/#blueprint.cli.main","title":"<code>main()</code>","text":"<p>Main entry point for the CLI application.</p> Source code in <code>src/blueprint/cli.py</code> <pre><code>def main():\n    \"\"\"Main entry point for the CLI application.\"\"\"\n    OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1\")\n    JAN_MODEL = os.getenv(\"JAN_MODEL\", \"llama 3.1\")\n\n    parser = argparse.ArgumentParser(\n        description=\"Generate git commit messages using LLMs.\"\n    )\n    parser.add_argument(\n        \"--ollama\",\n        action=\"store_true\",\n        help=\"Use Ollama API instead of Jan AI (default is Jan AI)\",\n    )\n    parser.add_argument(\n        \"--analytics\", action=\"store_true\", help=\"Display performance analytics\"\n    )\n    parser.add_argument(\n        \"--vim\", action=\"store_true\", help=\"Use vim-style navigation in fzf\"\n    )\n    parser.add_argument(\n        \"--num\", action=\"store_true\", help=\"Use number selection for commit messages\"\n    )\n    parser.add_argument(\n        \"--max_chars\",\n        type=int,\n        default=75,\n        help=\"Suggested maximum number of characters for each commit message (default: 75)\",\n    )\n    args = parser.parse_args()\n\n    # Start timing\n    start_time = time.time()\n\n    # Get git diff\n    diff = get_git_diff()\n    if not diff:\n        print(\"No changes to commit.\")\n        sys.exit(0)\n\n    # Generate commit messages\n    service_type = \"ollama\" if args.ollama else \"jan\"\n    commit_messages = generate_commit_messages(\n        diff=diff,\n        max_chars=args.max_chars,\n        service_type=service_type,\n        ollama_model=OLLAMA_MODEL,\n        jan_model=JAN_MODEL,\n    )\n\n    # Stop timing for initial generation\n    end_time = time.time()\n\n    # Show analytics if requested\n    if args.analytics:\n        print(f\"\\nAnalytics:\")\n        print(\n            f\"Time taken to generate commit messages: {end_time - start_time:.2f} seconds\"\n        )\n        print(f\"Inference used: {'Ollama' if args.ollama else 'Jan AI'}\")\n        print(f\"Model name: {OLLAMA_MODEL if args.ollama else JAN_MODEL}\")\n        print(\"\")  # Add a blank line for better readability\n\n    # Check if we have messages\n    if not commit_messages:\n        print(\"Error: Could not generate commit messages.\")\n        sys.exit(1)\n\n    # Select message or regenerate\n    while True:\n        selected_message = select_message_with_fzf(\n            commit_messages, use_vim=args.vim, use_num=args.num\n        )\n\n        if selected_message == \"regenerate\":\n            # Time regeneration\n            start_time = time.time()\n\n            commit_messages = generate_commit_messages(\n                diff=diff,\n                max_chars=args.max_chars,\n                service_type=service_type,\n                ollama_model=OLLAMA_MODEL,\n                jan_model=JAN_MODEL,\n            )\n\n            end_time = time.time()\n\n            if args.analytics:\n                print(f\"\\nRegeneration Analytics:\")\n                print(\n                    f\"Time taken to regenerate commit messages: {end_time - start_time:.2f} seconds\"\n                )\n                print(\"\")  # Add a blank line for better readability\n\n            if not commit_messages:\n                print(\"Error: Could not generate commit messages.\")\n                sys.exit(1)\n        elif selected_message:\n            create_commit(selected_message)\n            break\n        else:\n            print(\"Commit messages rejected. Please create commit message manually.\")\n            break\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-model","title":"\ud83e\udde0 Changing the Model","text":"<p>You can customize the AI models used by LLM Commit Generator:</p>"},{"location":"customization/#jan-ai-models-default","title":"Jan AI Models (Default)","text":"<p>Set your preferred Jan AI model using the environment variable:</p> <pre><code>export JAN_MODEL=\"llama 3.1\"  # or any other model you've downloaded\n</code></pre>"},{"location":"customization/#ollama-models","title":"Ollama Models","text":"<p>Set your preferred Ollama model using the environment variable:</p> <pre><code>export OLLAMA_MODEL=\"llama3.1\"  # or any other model you've pulled\n</code></pre>"},{"location":"customization/#modifying-the-system-prompt","title":"\ud83d\udcdd Modifying the System Prompt","text":"<p>You can customize the prompt sent to the AI models by modifying the <code>generate_commit_messages</code> function in the <code>commit_generator.py</code> file. This allows you to:</p> <ul> <li>Change the number of commit messages generated</li> <li>Adjust the style or format of the messages</li> <li>Add specific guidelines for your team's commit message standards</li> </ul>"},{"location":"customization/#other-customization-ideas","title":"\ud83d\udca1 Other Customization Ideas","text":"<ul> <li>Add support for additional AI providers</li> <li>Implement commit message templates</li> <li>Add integration with commit hooks</li> <li>Create organization-specific formatting rules</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you're an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <p>We'd love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you're inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started with LLM Commit Generator","text":"<p>Get started with this Blueprint using one of the options below:</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>Python 3.10 or higher installed</li> <li>Git installed and configured</li> <li>fzf installed for the terminal UI (see installation guide)</li> <li>Either:</li> <li>Jan AI installed for local inference (install guide)</li> <li>Or Ollama as an alternative local inference option (install guide)</li> </ol>"},{"location":"getting-started/#install-from-pypi","title":"Install from PyPI","text":"<p>The easiest way to install LLM Commit Generator is via pip:</p> <pre><code>pip install llm-commit-generator\n</code></pre>"},{"location":"getting-started/#install-from-source","title":"Install from Source","text":"<p>Alternatively, you can install from source:</p> <pre><code>git clone https://github.com/tooluse/llmcommit.git\ncd llmcommit\npip install -e .\n</code></pre>"},{"location":"getting-started/#setup","title":"Setup","text":""},{"location":"getting-started/#for-inference-with-jan-ai-default","title":"For Inference with Jan AI (Default)","text":"<ol> <li>Install and launch Jan AI from jan.ai</li> <li>Download a model (like llama 3.1) through the Jan AI interface</li> <li>Ensure the Jan AI application is running</li> <li>(Optional) Set a custom model:    <pre><code>export JAN_MODEL=\"your-preferred-model\"\n</code></pre></li> </ol>"},{"location":"getting-started/#for-inference-with-ollama","title":"For Inference with Ollama","text":"<ol> <li>Start the Ollama service</li> <li>Pull a model (we recommend llama3.1):    <pre><code>ollama pull llama3.1\n</code></pre></li> <li>(Optional) Set a custom model:    <pre><code>export OLLAMA_MODEL=\"your-preferred-model\"\n</code></pre></li> </ol>"},{"location":"getting-started/#usage","title":"Usage","text":""},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<ol> <li>Make changes to your files in a git repository</li> <li>Run:    <pre><code>lcg\n</code></pre></li> <li>Select one of the generated commit messages using the arrow keys or number keys</li> <li>Press Enter to commit with the selected message, or Esc to cancel</li> </ol>"},{"location":"getting-started/#advanced-options","title":"Advanced Options","text":"<pre><code># Use Ollama instead of Jan AI\nlcg --ollama\n\n# Show performance analytics\nlcg --analytics\n\n# Use vim-style navigation in fzf\nlcg --vim\n\n# Use number selection for messages\nlcg --num\n\n# Set the maximum characters for commit messages\nlcg --max_chars 100\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<ul> <li>See the Step-by-Step Guide to understand how LLM Commit Generator works</li> <li>Learn about customization options to tailor it to your needs</li> </ul>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How LLM Commit Generator Works","text":"<p>This guide explains exactly how LLM Commit Generator works, from extracting your git diff to generating commit messages and allowing you to select the best one.</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>LLM Commit Generator is a tool that leverages large language models to automatically generate meaningful git commit messages based on your code changes. It uses Jan AI by default for local inference, with Ollama as an alternative option, giving you flexibility while keeping your data private.</p> <p>The process follows these steps:</p> <ol> <li>Extract git diff from your repository</li> <li>Send the diff to an AI model for analysis</li> <li>Parse the AI's response into commit message options</li> <li>Present the options in a user-friendly interface</li> <li>Create a git commit with your selected message</li> </ol>"},{"location":"step-by-step-guide/#step-1-extracting-the-git-diff","title":"Step 1: Extracting the Git Diff","text":"<p>When you run <code>lcg</code>, the tool first tries to get the staged changes using <code>git diff --cached</code>. If there are no staged changes, it falls back to unstaged changes using <code>git diff</code>. This ensures that the AI model sees only the relevant changes you want to commit.</p> <p>The diff is limited to 5000 characters to avoid overwhelming the AI model and to respect the context windows of various models.</p> <pre><code>def get_git_diff(max_chars: int = 5000) -&gt; str:\n    try:\n        diff = subprocess.check_output([\"git\", \"diff\", \"--cached\"], text=True)\n        if not diff:\n            diff = subprocess.check_output([\"git\", \"diff\"], text=True)\n        return diff[:max_chars]\n    except subprocess.CalledProcessError:\n        print(\"Error: Not a git repository or git is not installed.\")\n        sys.exit(1)\n</code></pre>"},{"location":"step-by-step-guide/#step-2-generating-commit-messages","title":"Step 2: Generating Commit Messages","text":"<p>The extracted diff is sent to an AI model with a carefully crafted prompt that asks it to generate three concise, informative commit messages. The prompt specifies:</p> <ul> <li>Generate three options</li> <li>Make each message reflect the entire diff</li> <li>Keep messages concise (default 75 characters)</li> <li>Format with numbering for easy parsing</li> </ul> <p>Based on your configuration, the tool will use either:</p> <ul> <li>Jan AI for local inference (default)</li> <li>Ollama as an alternative local inference option</li> </ul>"},{"location":"step-by-step-guide/#step-3-presenting-options-to-user","title":"Step 3: Presenting Options to User","text":"<p>The AI's response is parsed to extract the suggested commit messages, which are then presented to you using the fzf terminal interface. This provides a clean, interactive way to browse and select the best message.</p> <p>Options include:</p> <ul> <li>Use arrow keys or vim-style navigation to select a message</li> <li>Use number keys for quick selection</li> <li>Option to regenerate messages if none are satisfactory</li> <li>Press Esc to cancel and write your own message</li> </ul>"},{"location":"step-by-step-guide/#step-4-creating-the-commit","title":"Step 4: Creating the Commit","text":"<p>After selecting a message, LLM Commit Generator uses <code>git commit -m \"your selected message\"</code> to create the commit with your chosen message.</p>"},{"location":"step-by-step-guide/#architecture","title":"Architecture","text":"<p>LLM Commit Generator is organized into several modules:</p> <ul> <li><code>ai_service.py</code>: Handles communication with AI providers (Jan AI/Ollama)</li> <li><code>commit_generator.py</code>: Core functions for generating and parsing commit messages</li> <li><code>cli.py</code>: Command-line interface and options handling</li> </ul> <p>This modular design makes it easy to extend and customize the tool for your specific needs.</p>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}