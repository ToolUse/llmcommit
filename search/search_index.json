{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Blueprint docs","text":"<p>Blueprints are customizable workflows that help developers build AI applications using open-source tools and models</p> <p>These docs are your companion to mastering this Blueprint.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Tool 1</li> <li>Tool 2</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-the-blueprint-in-minutes","title":"Start building the Blueprint in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor project parameters to fit your needs</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#ai-service-module","title":"AI Service Module","text":""},{"location":"api/#blueprint.ai_service","title":"<code>blueprint.ai_service</code>","text":"<p>AI Service module for interacting with different LLM providers.</p>"},{"location":"api/#blueprint.ai_service.AIService","title":"<code>AIService</code>","text":"<p>Service for interacting with different AI providers.</p> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>class AIService:\n    \"\"\"Service for interacting with different AI providers.\"\"\"\n\n    def __init__(\n        self, service_type: str, model: Optional[str] = None, debug: bool = False\n    ):\n        \"\"\"Initialize AI service.\n\n        Args:\n            service_type: Type of AI service ('ollama' or 'jan')\n            model: Model name to use\n            debug: Whether to enable debug logging\n        \"\"\"\n        self.service_type = service_type\n        self.model = model\n        self.debug = debug\n        self.logger = logging.getLogger(__name__)\n\n        # Set up base URLs for services\n        self.base_urls = {\n            \"ollama\": OLLAMA_BASE_URL,\n            \"jan\": JAN_BASE_URL,\n        }\n\n        if service_type not in self.base_urls:\n            self.logger.error(f\"Unsupported service type: {service_type}\")\n            raise ValueError(f\"Unsupported service type: {service_type}\")\n\n        self.logger.debug(\n            f\"Initialized AIService with {service_type} and model {model}\"\n        )\n\n    def query(self, prompt: str) -&gt; str:\n        \"\"\"Query the AI service with the given prompt.\n\n        Args:\n            prompt: The prompt to send to the AI service\n\n        Returns:\n            The response from the AI service\n\n        Raises:\n            Exception: If there's an error communicating with the AI service\n        \"\"\"\n        if self.service_type == \"ollama\":\n            return self._query_ollama(prompt)\n        elif self.service_type == \"jan\":\n            return self._query_jan(prompt)\n        else:\n            self.logger.error(f\"Unsupported service type: {self.service_type}\")\n            raise ValueError(f\"Unsupported service type: {self.service_type}\")\n\n    def _query_jan(self, prompt: str) -&gt; str:\n        \"\"\"Send query to Jan AI API.\n\n        Args:\n            prompt: The prompt text\n\n        Returns:\n            Generated text response\n        \"\"\"\n        url = self.base_urls[\"jan\"]\n        headers = {\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = {\n            \"model\": self.model,\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a commit message generator. You only summarize code in git diffs.\",\n                },\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": 0.2,\n        }\n\n        self.logger.debug(f\"Sending request to Jan AI API at {url}\")\n        if self.debug:\n            self.logger.debug(f\"Request data: {data}\")\n\n        try:\n            self.logger.debug(\"Making POST request to Jan AI API\")\n            # Add timeout parameter to prevent hanging\n            response = requests.post(url, headers=headers, json=data, timeout=60)\n            self.logger.debug(\n                f\"Received response with status code: {response.status_code}\"\n            )\n\n            if self.debug:\n                self.logger.debug(f\"Response headers: {response.headers}\")\n\n            response.raise_for_status()\n            result = response.json()\n\n            if not result.get(\"choices\") or not result[\"choices\"][0].get(\"message\"):\n                self.logger.error(f\"Unexpected response format from Jan AI: {result}\")\n                if self.debug:\n                    self.logger.debug(f\"Full response: {result}\")\n                return \"\"\n\n            content = result[\"choices\"][0][\"message\"][\"content\"]\n            return content\n        except requests.exceptions.ConnectionError as e:\n            self.logger.error(f\"Error connecting to Jan AI API: {e}\")\n            raise Exception(\n                \"Error connecting to Jan AI: Is Jan AI running on localhost:1337?\"\n            )\n        except requests.exceptions.Timeout as e:\n            self.logger.error(f\"Jan AI API request timed out: {e}\")\n            raise Exception(\"Jan AI request timed out. Service may be overloaded.\")\n        except Exception as e:\n            self.logger.error(f\"Error querying Jan AI API: {e}\")\n            raise Exception(f\"Error with Jan AI: {str(e)[:100]}\")\n\n    def _query_ollama(self, prompt: str) -&gt; str:\n        \"\"\"Send query to Ollama API.\n\n        Args:\n            prompt: The prompt text\n\n        Returns:\n            Generated text response\n        \"\"\"\n        url = self.base_urls[\"ollama\"]\n        data = {\"model\": self.model, \"prompt\": prompt, \"stream\": False}\n\n        self.logger.debug(f\"Sending request to Ollama API at {url}\")\n        if self.debug:\n            self.logger.debug(f\"Request data: {data}\")\n\n        try:\n            self.logger.debug(\"Making POST request to Ollama API\")\n            # Add timeout parameter to prevent hanging\n            response = requests.post(url, json=data, timeout=60)\n            self.logger.debug(\n                f\"Received response with status code: {response.status_code}\"\n            )\n\n            if self.debug:\n                self.logger.debug(f\"Response headers: {response.headers}\")\n\n            response.raise_for_status()\n            result = response.json()\n\n            if not result.get(\"response\"):\n                self.logger.error(f\"Unexpected response format from Ollama: {result}\")\n                if self.debug:\n                    self.logger.debug(f\"Full response: {result}\")\n\n            return result.get(\"response\", \"\")\n        except requests.exceptions.ConnectionError as e:\n            self.logger.error(f\"Error connecting to Ollama API: {e}\")\n            raise Exception(\n                \"Error connecting to Ollama: Is Ollama running on localhost:11434?\"\n            )\n        except requests.exceptions.Timeout as e:\n            self.logger.error(f\"Ollama API request timed out: {e}\")\n            raise Exception(\n                \"Ollama request timed out. Service may be overloaded or model is too large.\"\n            )\n        except Exception as e:\n            self.logger.error(f\"Error querying Ollama API: {e}\")\n            raise Exception(f\"Error with Ollama: {str(e)[:100]}\")\n</code></pre>"},{"location":"api/#blueprint.ai_service.AIService.__init__","title":"<code>__init__(service_type, model=None, debug=False)</code>","text":"<p>Initialize AI service.</p> <p>Parameters:</p> Name Type Description Default <code>service_type</code> <code>str</code> <p>Type of AI service ('ollama' or 'jan')</p> required <code>model</code> <code>Optional[str]</code> <p>Model name to use</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>def __init__(\n    self, service_type: str, model: Optional[str] = None, debug: bool = False\n):\n    \"\"\"Initialize AI service.\n\n    Args:\n        service_type: Type of AI service ('ollama' or 'jan')\n        model: Model name to use\n        debug: Whether to enable debug logging\n    \"\"\"\n    self.service_type = service_type\n    self.model = model\n    self.debug = debug\n    self.logger = logging.getLogger(__name__)\n\n    # Set up base URLs for services\n    self.base_urls = {\n        \"ollama\": OLLAMA_BASE_URL,\n        \"jan\": JAN_BASE_URL,\n    }\n\n    if service_type not in self.base_urls:\n        self.logger.error(f\"Unsupported service type: {service_type}\")\n        raise ValueError(f\"Unsupported service type: {service_type}\")\n\n    self.logger.debug(\n        f\"Initialized AIService with {service_type} and model {model}\"\n    )\n</code></pre>"},{"location":"api/#blueprint.ai_service.AIService.query","title":"<code>query(prompt)</code>","text":"<p>Query the AI service with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to send to the AI service</p> required <p>Returns:</p> Type Description <code>str</code> <p>The response from the AI service</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error communicating with the AI service</p> Source code in <code>src/blueprint/ai_service.py</code> <pre><code>def query(self, prompt: str) -&gt; str:\n    \"\"\"Query the AI service with the given prompt.\n\n    Args:\n        prompt: The prompt to send to the AI service\n\n    Returns:\n        The response from the AI service\n\n    Raises:\n        Exception: If there's an error communicating with the AI service\n    \"\"\"\n    if self.service_type == \"ollama\":\n        return self._query_ollama(prompt)\n    elif self.service_type == \"jan\":\n        return self._query_jan(prompt)\n    else:\n        self.logger.error(f\"Unsupported service type: {self.service_type}\")\n        raise ValueError(f\"Unsupported service type: {self.service_type}\")\n</code></pre>"},{"location":"api/#commit-generator-module","title":"Commit Generator Module","text":""},{"location":"api/#blueprint.commit_generator","title":"<code>blueprint.commit_generator</code>","text":"<p>Git commit message generator using AI models.</p>"},{"location":"api/#blueprint.commit_generator.create_commit","title":"<code>create_commit(message, debug=False)</code>","text":"<p>Create a git commit with the selected message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Commit message to use</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if commit was successful, False otherwise</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def create_commit(message: str, debug: bool = False) -&gt; bool:\n    \"\"\"Create a git commit with the selected message.\n\n    Args:\n        message: Commit message to use\n        debug: Whether to enable debug logging\n\n    Returns:\n        True if commit was successful, False otherwise\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(f\"Creating commit with message: '{message}'\")\n\n    try:\n        subprocess.run([\"git\", \"commit\", \"-m\", message], check=True)\n        logger.debug(\"Commit created successfully\")\n        print(f\"Committed with message: {message}\")\n        return True\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Failed to create commit: {e}\")\n        print(\"Error: Failed to create commit.\")\n        return False\n</code></pre>"},{"location":"api/#blueprint.commit_generator.filter_diff","title":"<code>filter_diff(raw_diff, include_filenames=True, debug=False)</code>","text":"<p>Filter git diff to remove metadata and keep only meaningful changes.</p> <p>Parameters:</p> Name Type Description Default <code>raw_diff</code> <code>str</code> <p>Raw git diff output</p> required <code>include_filenames</code> <code>bool</code> <p>Whether to keep filenames in the output</p> <code>True</code> <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Filtered diff with only relevant content</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def filter_diff(\n    raw_diff: str, include_filenames: bool = True, debug: bool = False\n) -&gt; str:\n    \"\"\"Filter git diff to remove metadata and keep only meaningful changes.\n\n    Args:\n        raw_diff: Raw git diff output\n        include_filenames: Whether to keep filenames in the output\n        debug: Whether to enable debug logging\n\n    Returns:\n        Filtered diff with only relevant content\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Filtering git diff to remove metadata\")\n\n    if not raw_diff:\n        return \"\"\n\n    filtered_lines = []\n    current_file = None\n\n    for line in raw_diff.split(\"\\n\"):\n        # Skip common metadata lines\n        if line.startswith(\"diff --git\") or line.startswith(\"index \"):\n            continue\n\n        # Handle filename markers but keep the filename\n        if line.startswith(\"--- \"):\n            continue\n        if line.startswith(\"+++ \"):\n            if line.startswith(\"+++ b/\") and include_filenames:\n                current_file = line[6:]  # Remove the \"+++ b/\" prefix\n            continue\n\n        # Add filename header if we just found a new file\n        if current_file and include_filenames:\n            filtered_lines.append(f\"File: {current_file}\")\n            current_file = None\n\n        # Keep everything else: hunk headers, context lines, and actual changes\n        filtered_lines.append(line)\n\n    filtered_diff = \"\\n\".join(filtered_lines)\n\n    if debug:\n        logger.debug(\n            f\"Original diff: {len(raw_diff)} chars, Filtered: {len(filtered_diff)} chars\"\n        )\n        logger.debug(f\"Removed {len(raw_diff) - len(filtered_diff)} chars of metadata\")\n        logger.debug(\n            \"Filtered diff preview (first 500 chars):\\n\" + filtered_diff[:500]\n            if filtered_diff\n            else \"(empty)\"\n        )\n\n    return filtered_diff\n</code></pre>"},{"location":"api/#blueprint.commit_generator.generate_commit_messages","title":"<code>generate_commit_messages(diff, max_chars=200, service_type='ollama', ollama_model='llama3.1', jan_model='llama3.2-3b-instruct', debug=False)</code>","text":"<p>Generate commit messages based on git diff.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>str</code> <p>Git diff to generate commit messages for</p> required <code>max_chars</code> <code>int</code> <p>Suggested maximum characters for commit messages</p> <code>200</code> <code>service_type</code> <code>str</code> <p>'ollama' or 'jan'</p> <code>'ollama'</code> <code>ollama_model</code> <code>str</code> <p>Model name for Ollama</p> <code>'llama3.1'</code> <code>jan_model</code> <code>str</code> <p>Model name for Jan AI</p> <code>'llama3.2-3b-instruct'</code> <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of generated commit messages</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def generate_commit_messages(\n    diff: str,\n    max_chars: int = 200,\n    service_type: str = \"ollama\",\n    ollama_model: str = \"llama3.1\",\n    jan_model: str = \"llama3.2-3b-instruct\",\n    debug: bool = False,\n) -&gt; List[str]:\n    \"\"\"Generate commit messages based on git diff.\n\n    Args:\n        diff: Git diff to generate commit messages for\n        max_chars: Suggested maximum characters for commit messages\n        service_type: 'ollama' or 'jan'\n        ollama_model: Model name for Ollama\n        jan_model: Model name for Jan AI\n        debug: Whether to enable debug logging\n\n    Returns:\n        List of generated commit messages\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Generating commit messages\")\n\n    # Filter the diff to remove noise\n    filtered_diff = filter_diff(diff, include_filenames=True, debug=debug)\n\n    # Explicit logging of the filtered diff for debugging\n    if debug:\n        logger.debug(f\"FILTERED DIFF used for prompting LLM:\\n{filtered_diff}\")\n        if not filtered_diff:\n            logger.warning(\"FILTERED DIFF is empty\")\n\n    prompt = get_commit_message_prompt(diff, max_chars)\n\n    logger.debug(f\"Created prompt with length {len(prompt)} chars\")\n    if debug:\n        logger.debug(\"FINAL PROMPT:\\n\" + prompt)\n\n    response = query_ai_service(\n        prompt, service_type, ollama_model, jan_model, debug=debug\n    )\n\n    if debug and response:\n        logger.debug(f\"Full response from LLM: {response}\")\n    elif not response:\n        logger.error(\"Received empty response from AI service\")\n\n    messages = parse_commit_messages(response, debug=debug)\n    logger.debug(f\"Generated {len(messages)} commit messages\")\n    return messages\n</code></pre>"},{"location":"api/#blueprint.commit_generator.get_git_diff","title":"<code>get_git_diff(max_chars=5000, debug=False)</code>","text":"<p>Get the git diff of staged changes, or unstaged if no staged changes. Filters out deleted files from the diff.</p> <p>Parameters:</p> Name Type Description Default <code>max_chars</code> <code>int</code> <p>Maximum number of characters to return</p> <code>5000</code> <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Git diff as string</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If not a git repository or git not installed</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def get_git_diff(max_chars: int = 5000, debug: bool = False) -&gt; str:\n    \"\"\"Get the git diff of staged changes, or unstaged if no staged changes.\n    Filters out deleted files from the diff.\n\n    Args:\n        max_chars: Maximum number of characters to return\n        debug: Whether to enable debug logging\n\n    Returns:\n        Git diff as string\n\n    Raises:\n        SystemExit: If not a git repository or git not installed\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        logger.debug(\"Checking for staged changes\")\n        diff = subprocess.check_output(\n            [\"git\", \"diff\", \"--cached\", \"--diff-filter=ACMTU\"], text=True\n        )\n        if not diff:\n            logger.debug(\"No staged changes found, checking for unstaged changes\")\n            diff = subprocess.check_output(\n                [\"git\", \"diff\", \"--diff-filter=ACMTU\"], text=True\n            )\n\n        # Use trim_diff to intelligently truncate if needed\n        if len(diff) &gt; max_chars:\n            diff = trim_diff(diff, max_chars, debug)\n\n        return diff\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"Git diff failed: {e}\")\n        print(\"Error: Not a git repository or git is not installed.\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#blueprint.commit_generator.parse_commit_messages","title":"<code>parse_commit_messages(response, debug=False)</code>","text":"<p>Parse the LLM response into a list of commit messages.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Text response from AI service</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of extracted commit messages</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def parse_commit_messages(response: str, debug: bool = False) -&gt; List[str]:\n    \"\"\"Parse the LLM response into a list of commit messages.\n\n    Args:\n        response: Text response from AI service\n        debug: Whether to enable debug logging\n\n    Returns:\n        List of extracted commit messages\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Parsing commit messages from AI response\")\n\n    messages = []\n    for line in response.split(\"\\n\"):\n        line = line.strip()\n        if debug:\n            logger.debug(f\"Processing line: {line}\")\n\n        if line.startswith((\"1.\", \"2.\", \"3.\")):\n            message = line.split(\".\", 1)[1].strip()\n            # Strip surrounding single quotes if present\n            if (message.startswith(\"'\") and message.endswith(\"'\")) or (\n                message.startswith('\"') and message.endswith('\"')\n            ):\n                message = message[1:-1]\n            messages.append(message)\n            logger.debug(f\"Extracted message: {message}\")\n\n    logger.debug(f\"Parsed {len(messages)} commit messages\")\n    return messages\n</code></pre>"},{"location":"api/#blueprint.commit_generator.query_ai_service","title":"<code>query_ai_service(prompt, service_type, ollama_model, jan_model, debug=False)</code>","text":"<p>Query AI service with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt text to send to AI service</p> required <code>service_type</code> <code>str</code> <p>Type of AI service ('ollama' or 'jan')</p> required <code>ollama_model</code> <code>str</code> <p>Model name for Ollama</p> required <code>jan_model</code> <code>str</code> <p>Model name for Jan AI</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Response from AI service</p> <p>Raises:</p> Type Description <code>SystemExit</code> <p>If there's an error querying the AI service</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def query_ai_service(\n    prompt: str,\n    service_type: str,\n    ollama_model: str,\n    jan_model: str,\n    debug: bool = False,\n) -&gt; str:\n    \"\"\"Query AI service with the given prompt.\n\n    Args:\n        prompt: Prompt text to send to AI service\n        service_type: Type of AI service ('ollama' or 'jan')\n        ollama_model: Model name for Ollama\n        jan_model: Model name for Jan AI\n        debug: Whether to enable debug logging\n\n    Returns:\n        Response from AI service\n\n    Raises:\n        SystemExit: If there's an error querying the AI service\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    try:\n        print(\"Generating commit messages...\", end=\"\", flush=True)\n        logger.debug(\n            f\"Querying {service_type} with model {ollama_model if service_type == 'ollama' else jan_model}\"\n        )\n\n        ai_service = AIService(\n            service_type,\n            model=ollama_model if service_type == \"ollama\" else jan_model,\n            debug=debug,\n        )\n\n        response = ai_service.query(prompt)\n        print(\"Done!\")\n\n        logger.debug(f\"Received response with length {len(response)} chars\")\n\n        return response\n    except Exception as e:\n        logger.error(f\"Error querying {service_type}: {e}\")\n        print(f\"\\nError querying {service_type.capitalize()}: {e}\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/#blueprint.commit_generator.trim_diff","title":"<code>trim_diff(diff, max_chars, debug=False)</code>","text":"<p>Intelligently trim a git diff to stay under max_chars by preserving complete files and hunks.</p> <p>Parameters:</p> Name Type Description Default <code>diff</code> <code>str</code> <p>The git diff to trim</p> required <code>max_chars</code> <code>int</code> <p>Maximum character limit</p> required <code>debug</code> <code>bool</code> <p>Whether to enable debug logging</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Trimmed diff with complete files and hunks</p> Source code in <code>src/blueprint/commit_generator.py</code> <pre><code>def trim_diff(diff: str, max_chars: int, debug: bool = False) -&gt; str:\n    \"\"\"Intelligently trim a git diff to stay under max_chars by preserving complete files and hunks.\n\n    Args:\n        diff: The git diff to trim\n        max_chars: Maximum character limit\n        debug: Whether to enable debug logging\n\n    Returns:\n        Trimmed diff with complete files and hunks\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(f\"Trimming diff to stay under {max_chars} chars\")\n\n    if len(diff) &lt;= max_chars:\n        return diff\n\n    lines = diff.split(\"\\n\")\n    result_lines: list[str] = []\n    current_length = 0\n    in_hunk = False\n\n    # First, count the number of actual change lines (+ or -) to prioritize\n    change_lines_count = 0\n    for line in lines:\n        stripped = line.lstrip()\n        if (stripped.startswith(\"+\") or stripped.startswith(\"-\")) and stripped not in (\n            \"+\",\n            \"-\",\n        ):\n            change_lines_count += 1\n\n    # If there are few changes, we want to keep ALL of them\n    keep_all_changes = change_lines_count &lt; 50  # arbitrary threshold\n    if keep_all_changes and debug:\n        logger.debug(\n            f\"Only {change_lines_count} actual change lines - will prioritize keeping all changes\"\n        )\n\n    # Initialize important indices set\n    important_indices: set[int] = set()\n\n    # First pass: collect critical changes and their context\n    if keep_all_changes:\n        for i, line in enumerate(lines):\n            stripped = line.lstrip()\n            # Mark change lines and surrounding context\n            if (\n                stripped.startswith(\"+\") or stripped.startswith(\"-\")\n            ) and stripped not in (\"+\", \"-\"):\n                # Mark this line and surrounding context (3 lines before and after)\n                for j in range(max(0, i - 3), min(len(lines), i + 4)):\n                    important_indices.add(j)\n            # Always mark hunk headers\n            elif stripped.startswith(\"@@\"):\n                important_indices.add(i)\n\n    # Second pass: keep important lines and natural boundaries\n    for i, line in enumerate(lines):\n        line_length = len(line) + 1  # +1 for newline\n        stripped = line.lstrip()\n\n        # Start of a new file\n        if line.startswith(\"diff --git\"):\n            # If adding this new file would exceed our limit, stop here\n            if current_length + line_length &gt; max_chars and result_lines:\n                # Unless this file contains important changes we want to keep\n                if keep_all_changes and any(\n                    j in important_indices for j in range(i, min(len(lines), i + 20))\n                ):\n                    if debug:\n                        logger.debug(\n                            f\"Keeping file at line {i} despite size limit due to important changes\"\n                        )\n                else:\n                    break\n            in_hunk = False\n\n        # Start of a new hunk\n        elif stripped.startswith(\"@@\"):\n            in_hunk = True\n\n        # If we're about to exceed the limit but this is an important line, keep it anyway\n        if current_length + line_length &gt; max_chars:\n            if keep_all_changes and i in important_indices:\n                if debug:\n                    logger.debug(f\"Keeping important line {i} despite size limit\")\n            # If we're not at a natural boundary and this isn't an important line, stop here\n            elif not in_hunk and not line.startswith(\"diff --git\"):\n                # We're between hunks or files, safe to stop here\n                break\n\n        # Add the line\n        result_lines.append(line)\n        current_length += line_length\n\n    result = \"\\n\".join(result_lines)\n\n    if debug:\n        logger.debug(f\"Trimmed diff from {len(diff)} chars to {len(result)} chars\")\n        logger.debug(f\"Preserved {len(result_lines)} of {len(lines)} lines\")\n        # Check if we preserved all important changes\n        if keep_all_changes:\n            preserved_important = sum(\n                1 for i in important_indices if i &lt; len(result_lines)\n            )\n            logger.debug(\n                f\"Preserved {preserved_important} of {len(important_indices)} important lines\"\n            )\n\n    return result\n</code></pre>"},{"location":"api/#cli-module","title":"CLI Module","text":""},{"location":"api/#blueprint.cli","title":"<code>blueprint.cli</code>","text":"<p>Command-line interface for LLM-powered commit message generator.</p>"},{"location":"api/#blueprint.cli.main","title":"<code>main()</code>","text":"<p>Main entry point for the CLI application.</p> Source code in <code>src/blueprint/cli.py</code> <pre><code>def main():\n    \"\"\"Main entry point for the CLI application.\"\"\"\n    OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", DEFAULT_OLLAMA_MODEL)\n    JAN_MODEL = os.getenv(\"JAN_MODEL\", DEFAULT_JAN_MODEL)\n\n    parser = argparse.ArgumentParser(\n        description=\"Generate git commit messages using LLMs.\"\n    )\n    parser.add_argument(\n        \"--ollama\",\n        action=\"store_true\",\n        help=\"Use Ollama API instead of Jan AI (default is Jan AI)\",\n    )\n    parser.add_argument(\n        \"--analytics\", action=\"store_true\", help=\"Display performance analytics\"\n    )\n    parser.add_argument(\n        \"--vim\", action=\"store_true\", help=\"Use vim-style navigation in fzf\"\n    )\n    parser.add_argument(\n        \"--num\", action=\"store_true\", help=\"Use number selection for commit messages\"\n    )\n    parser.add_argument(\n        \"--max_chars\",\n        type=int,\n        default=75,\n        help=\"Suggested maximum number of characters for each commit message (default: 75)\",\n    )\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n    args = parser.parse_args()\n\n    # Set up logging\n    setup_logging(args.debug)\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Debug mode enabled\")\n\n    # Start timing\n    start_time = time.time()\n\n    # Get git diff\n    logger.debug(\"Getting git diff\")\n    diff = get_git_diff(debug=args.debug)\n    if not diff:\n        logger.error(\"No changes to commit\")\n        print(\"No changes to commit.\")\n        sys.exit(0)\n\n    # Generate commit messages\n    service_type = \"ollama\" if args.ollama else \"jan\"\n    logger.debug(f\"Generating commit messages using {service_type}\")\n    commit_messages = generate_commit_messages(\n        diff=diff,\n        max_chars=args.max_chars,\n        service_type=service_type,\n        ollama_model=OLLAMA_MODEL,\n        jan_model=JAN_MODEL,\n        debug=args.debug,\n    )\n\n    # Stop timing for initial generation\n    end_time = time.time()\n\n    # Show analytics if requested\n    if args.analytics:\n        print(\"\\nAnalytics:\")\n        print(\n            f\"Time taken to generate commit messages: {end_time - start_time:.2f} seconds\"\n        )\n        print(f\"Inference used: {'Ollama' if args.ollama else 'Jan AI'}\")\n        print(f\"Model name: {OLLAMA_MODEL if args.ollama else JAN_MODEL}\")\n        print(\"\")  # Add a blank line for better readability\n\n    # Check if we have messages\n    if not commit_messages:\n        logger.error(\"Could not generate commit messages\")\n        print(\"Error: Could not generate commit messages.\")\n        sys.exit(1)\n\n    # Select message or regenerate\n    while True:\n        selected_message = select_message_with_fzf(\n            commit_messages, use_vim=args.vim, use_num=args.num\n        )\n\n        if selected_message == \"regenerate\":\n            # Time regeneration\n            start_time = time.time()\n            logger.debug(\"Regenerating commit messages\")\n\n            commit_messages = generate_commit_messages(\n                diff=diff,\n                max_chars=args.max_chars,\n                service_type=service_type,\n                ollama_model=OLLAMA_MODEL,\n                jan_model=JAN_MODEL,\n                debug=args.debug,\n            )\n\n            end_time = time.time()\n\n            if args.analytics:\n                print(\"\\nRegeneration Analytics:\")\n                print(\n                    f\"Time taken to regenerate commit messages: {end_time - start_time:.2f} seconds\"\n                )\n                print(\"\")  # Add a blank line for better readability\n\n            if not commit_messages:\n                logger.error(\"Could not regenerate commit messages\")\n                print(\"Error: Could not generate commit messages.\")\n                sys.exit(1)\n        elif selected_message:\n            logger.debug(f\"Creating commit with message: {selected_message}\")\n            create_commit(selected_message, debug=args.debug)\n            break\n        else:\n            logger.debug(\"Commit messages rejected\")\n            print(\"Commit messages rejected. Please create commit message manually.\")\n            break\n</code></pre>"},{"location":"api/#blueprint.cli.select_message_with_fzf","title":"<code>select_message_with_fzf(messages, use_vim=False, use_num=False)</code>","text":"<p>Use fzf to select a commit message, with option to regenerate.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[str]</code> <p>List of commit messages to select from</p> required <code>use_vim</code> <code>bool</code> <p>Whether to use vim-style navigation</p> <code>False</code> <code>use_num</code> <code>bool</code> <p>Whether to display numbers for selection</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Selected message, \"regenerate\" to regenerate messages, or None if cancelled</p> Source code in <code>src/blueprint/cli.py</code> <pre><code>def select_message_with_fzf(\n    messages: List[str],\n    use_vim: bool = False,\n    use_num: bool = False,\n) -&gt; Optional[str]:\n    \"\"\"Use fzf to select a commit message, with option to regenerate.\n\n    Args:\n        messages: List of commit messages to select from\n        use_vim: Whether to use vim-style navigation\n        use_num: Whether to display numbers for selection\n\n    Returns:\n        Selected message, \"regenerate\" to regenerate messages, or None if cancelled\n    \"\"\"\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Displaying fzf selector for commit messages\")\n\n    try:\n        messages.append(\"Regenerate messages\")\n        fzf_args = [\n            \"fzf\",\n            \"--height=10\",\n            \"--layout=reverse\",\n            \"--prompt=Select a commit message (ESC to cancel): \",\n            \"--no-info\",\n            \"--margin=1,2\",\n            \"--border\",\n            \"--color=prompt:#D73BC9,pointer:#D73BC9\",\n        ]\n\n        if use_vim:\n            fzf_args.extend([\"--bind\", \"j:down,k:up\"])\n            logger.debug(\"Using vim-style navigation in fzf\")\n\n        if use_num:\n            for i, msg in enumerate(messages):\n                messages[i] = f\"{i + 1}. {msg}\"\n            fzf_args.extend(\n                [\n                    \"--bind\",\n                    \"1:accept-non-empty,2:accept-non-empty,3:accept-non-empty,4:accept-non-empty\",\n                ]\n            )\n            logger.debug(\"Using number selection in fzf\")\n\n        logger.debug(f\"Displaying {len(messages)} options in fzf\")\n        result = subprocess.run(\n            fzf_args,\n            input=\"\\n\".join(messages),\n            capture_output=True,\n            text=True,\n        )\n        if result.returncode == 130:  # User pressed ESC\n            logger.debug(\"User cancelled selection with ESC\")\n            return None\n        selected = result.stdout.strip()\n        logger.debug(f\"User selected: '{selected}'\")\n\n        if selected == \"Regenerate messages\" or selected == \"4. Regenerate messages\":\n            logger.debug(\"User chose to regenerate messages\")\n            return \"regenerate\"\n\n        final_selection = (\n            selected.split(\". \", 1)[1] if use_num and selected else selected\n        )\n        logger.debug(f\"Final selection: '{final_selection}'\")\n        return final_selection\n    except subprocess.CalledProcessError as e:\n        logger.error(f\"fzf selection failed: {e}\")\n        print(\"Error: fzf selection failed.\")\n        return None\n</code></pre>"},{"location":"api/#blueprint.cli.setup_logging","title":"<code>setup_logging(debug_mode)</code>","text":"<p>Set up logging based on debug mode.</p> Source code in <code>src/blueprint/cli.py</code> <pre><code>def setup_logging(debug_mode):\n    \"\"\"Set up logging based on debug mode.\"\"\"\n    log_level = logging.DEBUG if debug_mode else logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format=\"%(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n</code></pre>"},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>This Blueprint is designed to be flexible and easily adaptable to your specific needs. This guide will walk you through some key areas you can customize to make the Blueprint your own.</p>"},{"location":"customization/#changing-the-model","title":"\ud83e\udde0 Changing the Model","text":"<p>You can customize the AI models used by LLM Commit Generator:</p>"},{"location":"customization/#jan-ai-models-default","title":"Jan AI Models (Default)","text":"<p>Set your preferred Jan AI model using the environment variable:</p> <pre><code>export JAN_MODEL=\"llama 3.1\"  # or any other model you've downloaded\n</code></pre>"},{"location":"customization/#ollama-models","title":"Ollama Models","text":"<p>Set your preferred Ollama model using the environment variable:</p> <pre><code>export OLLAMA_MODEL=\"llama3.1\"  # or any other model you've pulled\n</code></pre>"},{"location":"customization/#modifying-the-system-prompt","title":"\ud83d\udcdd Modifying the System Prompt","text":"<p>You can customize the prompt sent to the AI models by modifying the <code>generate_commit_messages</code> function in the <code>commit_generator.py</code> file. This allows you to:</p> <ul> <li>Change the number of commit messages generated</li> <li>Adjust the style or format of the messages</li> <li>Add specific guidelines for your team's commit message standards</li> </ul>"},{"location":"customization/#other-customization-ideas","title":"\ud83d\udca1 Other Customization Ideas","text":"<ul> <li>Add support for additional AI providers</li> <li>Implement commit message templates</li> <li>Add integration with commit hooks</li> <li>Create organization-specific formatting rules</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>This Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you're an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <p>We'd love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Discussions.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you're inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started with LLM Commit Generator","text":"<p>Get started with this Blueprint using one of the options below:</p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have:</p> <ol> <li>Python 3.10 or higher installed</li> <li>Git installed and configured</li> <li>fzf installed for the terminal UI (see installation guide)</li> <li>Either:</li> <li>Jan AI installed for local inference (install guide)</li> <li>Or Ollama as an alternative local inference option (install guide)</li> </ol>"},{"location":"getting-started/#install-from-pypi","title":"Install from PyPI","text":"<p>The easiest way to install LLM Commit Generator is via pip:</p> <pre><code>pip install llm-commit-generator\n</code></pre>"},{"location":"getting-started/#install-from-source","title":"Install from Source","text":"<p>Alternatively, you can install from source:</p> <pre><code>git clone https://github.com/tooluse/llmcommit.git\ncd llmcommit\npip install -e .\n</code></pre>"},{"location":"getting-started/#setup","title":"Setup","text":""},{"location":"getting-started/#for-inference-with-jan-ai-default","title":"For Inference with Jan AI (Default)","text":"<ol> <li>Install and launch Jan AI from jan.ai</li> <li>Download a model (like llama 3.1) through the Jan AI interface</li> <li>Ensure the Jan AI application is running</li> <li>(Optional) Set a custom model:    <pre><code>export JAN_MODEL=\"your-preferred-model\"\n</code></pre></li> </ol>"},{"location":"getting-started/#for-inference-with-ollama","title":"For Inference with Ollama","text":"<ol> <li>Start the Ollama service</li> <li>Pull a model (we recommend llama3.1):    <pre><code>ollama pull llama3.1\n</code></pre></li> <li>(Optional) Set a custom model:    <pre><code>export OLLAMA_MODEL=\"your-preferred-model\"\n</code></pre></li> </ol>"},{"location":"getting-started/#usage","title":"Usage","text":""},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<ol> <li>Make changes to your files in a git repository</li> <li>Run:    <pre><code>lcm\n</code></pre></li> <li>Select one of the generated commit messages using the arrow keys or number keys</li> <li>Press Enter to commit with the selected message, or Esc to cancel</li> </ol>"},{"location":"getting-started/#advanced-options","title":"Advanced Options","text":"<pre><code># Use Ollama instead of Jan AI\nlcm --ollama\n\n# Show performance analytics\nlcm --analytics\n\n# Use vim-style navigation in fzf\nlcm --vim\n\n# Use number selection for messages\nlcm --num\n\n# Set the maximum characters for commit messages\nlcm --max_chars 100\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<ul> <li>See the Step-by-Step Guide to understand how LLM Commit Generator works</li> <li>Learn about customization options to tailor it to your needs</li> </ul>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How LLM Commit Generator Works","text":"<p>This guide explains exactly how LLM Commit Generator works, from extracting your git diff to generating commit messages and allowing you to select the best one.</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>LLM Commit Generator is a tool that leverages large language models to automatically generate meaningful git commit messages based on your code changes. It uses Jan AI by default for local inference, with Ollama as an alternative option, giving you flexibility while keeping your data private.</p> <p>The process follows these steps:</p> <ol> <li>Extract git diff from your repository</li> <li>Send the diff to an AI model for analysis</li> <li>Parse the AI's response into commit message options</li> <li>Present the options in a user-friendly interface</li> <li>Create a git commit with your selected message</li> </ol>"},{"location":"step-by-step-guide/#step-1-extracting-the-git-diff","title":"Step 1: Extracting the Git Diff","text":"<p>When you run <code>lcm</code>, the tool first tries to get the staged changes using <code>git diff --cached</code>. If there are no staged changes, it falls back to unstaged changes using <code>git diff</code>. This ensures that the AI model sees only the relevant changes you want to commit.</p> <p>The diff is limited to 5000 characters to avoid overwhelming the AI model and to respect the context windows of various models.</p> <pre><code>def get_git_diff(max_chars: int = 5000) -&gt; str:\n    try:\n        diff = subprocess.check_output([\"git\", \"diff\", \"--cached\"], text=True)\n        if not diff:\n            diff = subprocess.check_output([\"git\", \"diff\"], text=True)\n        return diff[:max_chars]\n    except subprocess.CalledProcessError:\n        print(\"Error: Not a git repository or git is not installed.\")\n        sys.exit(1)\n</code></pre>"},{"location":"step-by-step-guide/#step-2-generating-commit-messages","title":"Step 2: Generating Commit Messages","text":"<p>The extracted diff is sent to an AI model with a carefully crafted prompt that asks it to generate three concise, informative commit messages. The prompt specifies:</p> <ul> <li>Generate three options</li> <li>Make each message reflect the entire diff</li> <li>Keep messages concise (default 75 characters)</li> <li>Format with numbering for easy parsing</li> </ul> <p>Based on your configuration, the tool will use either:</p> <ul> <li>Jan AI for local inference (default)</li> <li>Ollama as an alternative local inference option</li> </ul>"},{"location":"step-by-step-guide/#step-3-presenting-options-to-user","title":"Step 3: Presenting Options to User","text":"<p>The AI's response is parsed to extract the suggested commit messages, which are then presented to you using the fzf terminal interface. This provides a clean, interactive way to browse and select the best message.</p> <p>Options include:</p> <ul> <li>Use arrow keys or vim-style navigation to select a message</li> <li>Use number keys for quick selection</li> <li>Option to regenerate messages if none are satisfactory</li> <li>Press Esc to cancel and write your own message</li> </ul>"},{"location":"step-by-step-guide/#step-4-creating-the-commit","title":"Step 4: Creating the Commit","text":"<p>After selecting a message, LLM Commit Generator uses <code>git commit -m \"your selected message\"</code> to create the commit with your chosen message.</p>"},{"location":"step-by-step-guide/#architecture","title":"Architecture","text":"<p>LLM Commit Generator is organized into several modules:</p> <ul> <li><code>ai_service.py</code>: Handles communication with AI providers (Jan AI/Ollama)</li> <li><code>commit_generator.py</code>: Core functions for generating and parsing commit messages</li> <li><code>cli.py</code>: Command-line interface and options handling</li> </ul> <p>This modular design makes it easy to extend and customize the tool for your specific needs.</p>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}